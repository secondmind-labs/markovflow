:py:mod:`markovflow.models.sparse_pep`
======================================

.. py:module:: markovflow.models.sparse_pep

.. autoapi-nested-parse::

   Module containing a model for CVI



Module Contents
---------------

.. py:class:: SparsePowerExpectationPropagation(kernel: markovflow.kernels.SDEKernel, inducing_points: tensorflow.Tensor, likelihood: markovflow.likelihoods.PEPScalarLikelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None, learning_rate=1.0, alpha=1.0)

   Bases: :py:obj:`markovflow.models.models.MarkovFlowSparseModel`

   This is the  Sparse Power Expectation Propagation Algorithm

   Approximates a the posterior of a model with GP prior and a general likelihood
   using a Gaussian posterior parameterized with Gaussian sites on
   inducing states u at inducing points z.

   The following notation is used:

       * x - the time points of the training data.
       * z - the time points of the inducing/pseudo points.
       * y - observations corresponding to time points x.
       * s(.) - the continuous time latent state process
       * u = s(z) - the discrete inducing latent state space model
       * f(.) - the noise free predictions of the model
       * p(y | f) - the likelihood
       * t(u) - a site (indices will refer to the associated data point)
       * p(.) the prior distribution
       * q(.) the variational distribution

   We use the state space formulation of Markovian Gaussian Processes that specifies:
   the conditional density of neighbouring latent states: p(s‚Çñ‚Çä‚ÇÅ| s‚Çñ)
   how to read out the latent process from these states: f‚Çñ = H s‚Çñ

   The likelihood links data to the latent process and p(y‚Çñ | f‚Çñ).
   We would like to approximate the posterior over the latent state space model of this model.

   To approximate the posterior, we maximise the evidence lower bound (ELBO) (‚Ñí) with
   respect to the parameters of the variational distribution, since::

       log p(y) = ‚Ñí(q) + KL[q(s) ‚Äñ p(s | y)]

   ...where::

       ‚Ñí(q) = ‚à´ log(p(s, y) / q(s)) q(s) ds

   We parameterize the variational posterior through M sites t‚Çò(v‚Çò)

       q(s) = p(s) ‚àè‚Çò  t‚Çò(v‚Çò)

   where t‚Çò(v‚Çò) are multivariate Gaussian sites on v‚Çò = [u‚Çò, u‚Çò‚Çä‚ÇÅ],
   i.e. consecutive inducing states.

   The sites are parameterized in the natural form

       t(v) = exp(ùúΩ·µÄœÜ(v) - A(ùúΩ)), where ùúΩ=[Œ∏‚ÇÅ, Œ∏‚ÇÇ] and ùõó(u)=[v, v·µÄv]

   with ùõó(v) are the sufficient statistics and ùúΩ the natural parameters

   :param kernel: A kernel that defines a prior over functions.
   :param inducing_points: The points in time on which inference should be performed,
       with shape ``batch_shape + [num_inducing]``.
   :param likelihood: A likelihood.
   :param mean_function: The mean function for the GP. Defaults to no mean function.
   :param learning_rate: the learning rate
   :param alpha: power as in Power Expectation Propagation

   .. py:method:: posterior()

      Posterior Process 


   .. py:method:: mask_indices(exclude_indices)

      Binary mask to exclude data indices
      :param exclude_indices:


   .. py:method:: back_project_nats(nat1, nat2, time_points)

      back project natural gradient associated to time points to their associated
      inducing sites.


   .. py:method:: local_objective(Fmu, Fvar, Y)

      Local objective of the PEP algorithm : log E_q(f) p(y|f)·µÉ 


   .. py:method:: local_objective_gradients(fx_mus, fx_covs, observations, alpha=1.0)

      Gradients of the local objective of the PEP algorithm wrt to the predictive mean 


   .. py:method:: fraction_sites(time_points)

      for all segment indexed m of consecutive inducing points [z_m, z_m+1[,
      this counts the time points t falling in that segment:
      c(m) = #{t, z_m <= t < z_m+1} and returns 1/c(m) or 0 when c(m)=0

      :param time_points: tensor of shape batch_shape + [num_data]
      :return: tensor of shape batch_shape + [num_data]


   .. py:method:: compute_posterior_ssm(nat1, nat2)

      Computes the variational posterior distribution on the vector of inducing states


   .. py:method:: dist_q()
      :property:

      Computes the variational posterior distribution on the vector of inducing states


   .. py:method:: compute_marginals()

      Compute pairwise marginals


   .. py:method:: remove_cavity_from_marginals(time_points, marginals)

      Remove cavity from marginals
      :param time_points:
      :param marginals: pairwise mean and covariance tensors


   .. py:method:: compute_cavity_state(time_points)

      The cavity distributions for data points at input time_points.
      This corresponds to the marginal distribution q·ê†‚Åø(f‚Çô) of q·ê†‚Åø(s) = q(s)/t‚Çò(v‚Çò)·µù·µÉ,
      where Œ≤ = a * (1 / #time points `touching` site t‚Çò)


   .. py:method:: compute_cavity(time_points)

      Cavity on f
      :param time_points: time points


   .. py:method:: compute_new_sites(input_data)

      Compute the site updates and perform one update step.
      :param input_data: A tuple of time points and observations containing the data from which
          to calculate the the updates:
          a tensor of inputs with shape ``batch_shape + [num_data]``,
          a tensor of observations with shape ``batch_shape + [num_data, observation_dim]``.


   .. py:method:: compute_log_norm(input_data)

      Compute the site updates and perform one update step.
      :param input_data: A tuple of time points and observations containing the data from which
          to calculate the the updates:
          a tensor of inputs with shape ``batch_shape + [num_data]``,
          a tensor of observations with shape ``batch_shape + [num_data, observation_dim]``.


   .. py:method:: compute_num_data_per_interval(time_points)

      compute fraction of site per data point 


   .. py:method:: compute_fraction(time_points)

      compute fraction of site per data point 


   .. py:method:: update_sites(input_data)

      apply updates 


   .. py:method:: energy(input_data)

      The PEP energy : ‚à´ ds p(s) ùö∑_m t_m(v_m)
      :param input_data: input data


   .. py:method:: loss(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> tensorflow.Tensor

      Return the loss, which is the negative evidence lower bound (ELBO).

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model.


   .. py:method:: dist_p() -> markovflow.gauss_markov.GaussMarkovDistribution
      :property:

      Return the prior `GaussMarkovDistribution`.


   .. py:method:: kernel() -> markovflow.kernels.SDEKernel
      :property:

      Return the kernel of the GP.


   .. py:method:: classic_elbo(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor])

      Computes the ELBO the classic way:
          ‚Ñí(q) = Œ£·µ¢ ‚à´ log(p(y·µ¢ | f)) q(f) df - KL[q(f) ‚Äñ p(f)]

      Note: this is mostly for testing purposes and not to be used for optimization

      :param input_data: A tuple of time points and observations
      :return: A scalar tensor representing the ELBO.


   .. py:method:: predict_log_density(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], full_output_cov: bool = False) -> tensorflow.Tensor

      Compute the log density of the data at the new data points.

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:
          a tensor of inputs with shape ``batch_shape + [num_data]``,
          a tensor of observations with shape ``batch_shape + [num_data, observation_dim]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).




:py:mod:`markovflow.models.variational_cvi`
===========================================

.. py:module:: markovflow.models.variational_cvi

.. autoapi-nested-parse::

   Module containing a model for CVI.



Module Contents
---------------

.. py:class:: GaussianProcessWithSitesBase(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], kernel: markovflow.kernels.SDEKernel, likelihood: gpflow.likelihoods.Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`markovflow.models.models.MarkovFlowModel`

   Base class for site-based Gaussian Process approximation such as EP and CVI.

   The following notation is used:

       * :math:`x` - the time points of the training data
       * :math:`y` - observations corresponding to time points :math:`x`
       * :math:`s(.)` - the latent state of the Markov chain
       * :math:`f(.)` - the noise free predictions of the model
       * :math:`p(y | f)` - the likelihood
       * :math:`t(f)` - a site (indices will refer to the associated data point)
       * :math:`p(.)` - the prior distribution
       * :math:`q(.)` - the variational distribution

   We use the state space formulation of Markovian Gaussian Processes that specifies:

   * The conditional density of neighbouring latent states :math:`p(sₖ₊₁| sₖ)`
   * How to read out the latent process from these states :math:`fₖ = H sₖ`

   The likelihood links data to the latent process and :math:`p(yₖ | fₖ)`.
   We would like to approximate the posterior over the latent state space model of this model.

   We parameterize the approximate posterior using sites :math:`tₖ(fₖ)`:

   .. math:: q(s) = p(s) ∏ₖ tₖ(fₖ)

   ...where :math:`tₖ(fₖ)` are univariate Gaussian sites parameterized in the natural form:

   .. math:: t(f) = exp(𝜽ᵀφ(f) - A(𝜽))

   ...and where :math:`𝜽=[θ₁,θ₂]` and :math:`𝛗(f)=[f,f²]`.

   Here, :math:`𝛗(f)` are the sufficient statistics and :math:`𝜽` are the natural parameters.


   :param input_data: A tuple containing the observed data:

       * Time points of observations with shape ``batch_shape + [num_data]``
       * Observations with shape ``batch_shape + [num_data, observation_dim]``

   :param kernel: A kernel that defines a prior over functions.
   :param likelihood: A likelihood with shape ``batch_shape + [num_inducing]``.
   :param mean_function: The mean function for the GP. Defaults to no mean function.

   .. py:method:: dist_q() -> markovflow.state_space_model.StateSpaceModel
      :property:

      Construct the :class:`~markovflow.state_space_model.StateSpaceModel` representation of
      the posterior process indexed at the time points.


   .. py:method:: posterior_kalman() -> markovflow.kalman_filter.KalmanFilterWithSites
      :property:

      Build the Kalman filter object from the prior state space models and the sites.


   .. py:method:: posterior()
      :property:

      Posterior object to predict outside of the training time points 


   .. py:method:: log_likelihood() -> tensorflow.Tensor

      Calculate :math:`log p(y)`.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: time_points() -> tensorflow.Tensor
      :property:

      Return the time points of the observations.

      :return: A tensor with shape ``batch_shape + [num_data]``.


   .. py:method:: conditioning_points() -> tensorflow.Tensor
      :property:

      Return the time points of the observations.

      :return: A tensor with shape ``batch_shape + [num_data]``.


   .. py:method:: observations() -> tensorflow.Tensor
      :property:

      Return the observations.

      :return: A tensor with shape ``batch_shape + [num_data, observation_dim]``.


   .. py:method:: kernel() -> markovflow.kernels.SDEKernel
      :property:

      Return the kernel.


   .. py:method:: likelihood() -> gpflow.likelihoods.Likelihood
      :property:

      Return the likelihood.


   .. py:method:: mean_function() -> markovflow.mean_function.MeanFunction
      :property:

      Return the mean function.


   .. py:method:: dist_p() -> markovflow.state_space_model.StateSpaceModel
      :property:

      Return the prior Gauss-Markov distribution.


   .. py:method:: loss() -> tensorflow.Tensor

      Return the loss, which is the negative ELBO.



.. py:class:: CVIGaussianProcess(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], kernel: markovflow.kernels.SDEKernel, likelihood: gpflow.likelihoods.Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None, learning_rate=0.1)

   Bases: :py:obj:`GaussianProcessWithSitesBase`

   Provides an alternative parameterization to a
   :class:`~markovflow.models.variational.VariationalGaussianProcess`.

   This class approximates the posterior of a model with a GP prior and a general likelihood
   using a Gaussian posterior parameterized with Gaussian sites.

   The following notation is used:

       * :math:`x` - the time points of the training data
       * :math:`y` - observations corresponding to time points :math:`x`
       * :math:`s(.)` - the latent state of the Markov chain
       * :math:`f(.)` - the noise free predictions of the model
       * :math:`p(y | f)` - the likelihood
       * :math:`t(f)` - a site (indices will refer to the associated data point)
       * :math:`p(.)` the prior distribution
       * :math:`q(.)` the variational distribution

   We use the state space formulation of Markovian Gaussian Processes that specifies:

   * The conditional density of neighbouring latent states :math:`p(sₖ₊₁| sₖ)`
   * How to read out the latent process from these states :math:`fₖ = H sₖ`

   The likelihood links data to the latent process and :math:`p(yₖ | fₖ)`.
   We would like to approximate the posterior over the latent state space model of this model.

   To approximate the posterior, we maximise the evidence lower bound (ELBO) :math:`ℒ` with
   respect to the parameters of the variational distribution, since:

   .. math:: log p(y) = ℒ(q) + KL[q(s) ‖ p(s | y)]

   ...where:

   .. math:: ℒ(q) = ∫ log(p(s, y) / q(s)) q(s) ds

   We parameterize the variational posterior through sites :math:`tₖ(fₖ)`:

   .. math:: q(s) = p(s) ∏ₖ tₖ(fₖ)

   ...where :math:`tₖ(fₖ)` are univariate Gaussian sites parameterized in the natural form:

   .. math:: t(f) = exp(𝜽ᵀφ(f) - A(𝜽))

   ...and where :math:`𝜽=[θ₁,θ₂]` and :math:`𝛗(f)=[f,f²]`.

   Here, :math:`𝛗(f)` are the sufficient statistics and :math:`𝜽` are the natural parameters.
   Note that the subscript :math:`k` has been omitted for simplicity.

   The natural gradient update of the sites can be shown to be the gradient of the
   variational expectations:

   .. math:: 𝐠 = ∇[𝞰][∫ log(p(y=Y|f)) q(f) df]

   ...with respect to the expectation parameters:

   .. math:: 𝞰 = E[𝛗(f)] = [μ, σ² + μ²]

   That is, :math:`𝜽 ← ρ𝜽 + (1-ρ)𝐠`, where :math:`ρ` is the learning rate.

   The key reference is::

     @inproceedings{khan2017conjugate,
       title={Conjugate-Computation Variational Inference: Converting Variational Inference
              in Non-Conjugate Models to Inferences in Conjugate Models},
       author={Khan, Mohammad and Lin, Wu},
       booktitle={Artificial Intelligence and Statistics},
       pages={878--887},
       year={2017}
     }

   :param input_data: A tuple containing the observed data:

       * Time points of observations with shape ``batch_shape + [num_data]``
       * Observations with shape ``batch_shape + [num_data, observation_dim]``

   :param kernel: A kernel that defines a prior over functions.
   :param likelihood: A likelihood with shape ``batch_shape + [num_inducing]``.
   :param mean_function: The mean function for the GP. Defaults to no mean function.
   :param learning_rate: The learning rate of the algorithm.

   .. py:method:: local_objective(Fmu: tensorflow.Tensor, Fvar: tensorflow.Tensor, Y: tensorflow.Tensor) -> tensorflow.Tensor

      Calculate local loss in CVI.

      :param Fmu: Means with shape ``[..., latent_dim]``.
      :param Fvar: Variances with shape ``[..., latent_dim]``.
      :param Y: Observations with shape ``[..., observation_dim]``.
      :return: A local objective with shape ``[...]``.


   .. py:method:: local_objective_and_gradients(Fmu: tensorflow.Tensor, Fvar: tensorflow.Tensor) -> tensorflow.Tensor

      Return the local objective and its gradients with regard to the expectation parameters.

      :param Fmu: Means :math:`μ` with shape ``[..., latent_dim]``.
      :param Fvar: Variances :math:`σ²` with shape ``[..., latent_dim]``.
      :return: A local objective and gradient with regard to :math:`[μ, σ² + μ²]`.


   .. py:method:: update_sites() -> None

      Perform one joint update of the Gaussian sites. That is:

      .. math:: 𝜽 ← ρ𝜽 + (1-ρ)𝐠


   .. py:method:: elbo() -> tensorflow.Tensor

      Calculate the evidence lower bound (ELBO) :math:`log p(y)`.

      This is done by computing the marginal of the model in which the likelihood terms were
      replaced by the Gaussian sites.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: classic_elbo() -> tensorflow.Tensor

      Compute the ELBO the classic way. That is:

      .. math:: ℒ(q) = Σᵢ ∫ log(p(yᵢ | f)) q(f) df - KL[q(f) ‖ p(f)]

      .. note:: This is mostly for testing purposes and should not be used for optimization.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: predict_log_density(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], full_output_cov: bool = False) -> tensorflow.Tensor

      Compute the log density of the data at the new data points.
      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:
          a tensor of inputs with shape ``batch_shape + [num_data]``,
          a tensor of observations with shape ``batch_shape + [num_data, observation_dim]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).



.. py:function:: back_project_nats(nat1, nat2, C)

   Transforms the natural parameters 𝜽f of a Gaussian with sufficient statistics 𝛗(f)=[f,f²]
   into equivalent rank one natural parameters 𝜽g of a (thus degenerate) Gaussian
   with sufficient statistics 𝛗(g)=[g,ggᵀ] where f = Cg.

   In practice  [θg₁, θg₂] = [θf₁ C,θf₂ CᵀC]

   :param nat1: natural parameters with size (num_time_points, 1)
   :param nat2: natural parameters with size (num_time_points, 1)
   :param C: projection with size (num_time_points, 1, project_dim)
   :return: natural parameters with size
           (num_time_points, project_dim)
           (num_time_points, project_dim, project_dim)



.. py:function:: gradient_transformation_mean_var_to_expectation(inputs: Tuple[tensorflow.Tensor, tensorflow.Tensor], grads: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

   Transform gradients.

   This is from :math:`𝐠` of a function with regard to :math:`[μ, σ²]` into its gradients
   with regard to :math:`[μ, σ² + μ²]`.

   :param inputs: Means and variances :math:`[μ, σ²]`.
   :param grads: Gradients :math:`𝐠`.



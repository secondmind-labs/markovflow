:py:mod:`markovflow.models.variational_cvi`
===========================================

.. py:module:: markovflow.models.variational_cvi

.. autoapi-nested-parse::

   Module containing a model for CVI.



Module Contents
---------------

.. py:class:: GaussianProcessWithSitesBase(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], kernel: markovflow.kernels.SDEKernel, likelihood: gpflow.likelihoods.Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`markovflow.models.models.MarkovFlowModel`

   Base class for site-based Gaussian Process approximation such as EP and CVI.

   The following notation is used:

       * :math:`x` - the time points of the training data
       * :math:`y` - observations corresponding to time points :math:`x`
       * :math:`s(.)` - the latent state of the Markov chain
       * :math:`f(.)` - the noise free predictions of the model
       * :math:`p(y | f)` - the likelihood
       * :math:`t(f)` - a site (indices will refer to the associated data point)
       * :math:`p(.)` - the prior distribution
       * :math:`q(.)` - the variational distribution

   We use the state space formulation of Markovian Gaussian Processes that specifies:

   * The conditional density of neighbouring latent states :math:`p(sâ‚–â‚Šâ‚| sâ‚–)`
   * How to read out the latent process from these states :math:`fâ‚– = H sâ‚–`

   The likelihood links data to the latent process and :math:`p(yâ‚– | fâ‚–)`.
   We would like to approximate the posterior over the latent state space model of this model.

   We parameterize the approximate posterior using sites :math:`tâ‚–(fâ‚–)`:

   .. math:: q(s) = p(s) âˆâ‚– tâ‚–(fâ‚–)

   ...where :math:`tâ‚–(fâ‚–)` are univariate Gaussian sites parameterized in the natural form:

   .. math:: t(f) = exp(ğœ½áµ€Ï†(f) - A(ğœ½))

   ...and where :math:`ğœ½=[Î¸â‚,Î¸â‚‚]` and :math:`ğ›—(f)=[f,fÂ²]`.

   Here, :math:`ğ›—(f)` are the sufficient statistics and :math:`ğœ½` are the natural parameters.


   :param input_data: A tuple containing the observed data:

       * Time points of observations with shape ``batch_shape + [num_data]``
       * Observations with shape ``batch_shape + [num_data, observation_dim]``

   :param kernel: A kernel that defines a prior over functions.
   :param likelihood: A likelihood with shape ``batch_shape + [num_inducing]``.
   :param mean_function: The mean function for the GP. Defaults to no mean function.

   .. py:method:: dist_q() -> markovflow.state_space_model.StateSpaceModel
      :property:

      Construct the :class:`~markovflow.state_space_model.StateSpaceModel` representation of
      the posterior process indexed at the time points.


   .. py:method:: posterior_kalman() -> markovflow.kalman_filter.KalmanFilterWithSites
      :property:

      Build the Kalman filter object from the prior state space models and the sites.


   .. py:method:: posterior()
      :property:

      Posterior object to predict outside of the training time points 


   .. py:method:: log_likelihood() -> tensorflow.Tensor

      Calculate :math:`log p(y)`.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: time_points() -> tensorflow.Tensor
      :property:

      Return the time points of the observations.

      :return: A tensor with shape ``batch_shape + [num_data]``.


   .. py:method:: conditioning_points() -> tensorflow.Tensor
      :property:

      Return the time points of the observations.

      :return: A tensor with shape ``batch_shape + [num_data]``.


   .. py:method:: observations() -> tensorflow.Tensor
      :property:

      Return the observations.

      :return: A tensor with shape ``batch_shape + [num_data, observation_dim]``.


   .. py:method:: kernel() -> markovflow.kernels.SDEKernel
      :property:

      Return the kernel.


   .. py:method:: likelihood() -> gpflow.likelihoods.Likelihood
      :property:

      Return the likelihood.


   .. py:method:: mean_function() -> markovflow.mean_function.MeanFunction
      :property:

      Return the mean function.


   .. py:method:: dist_p() -> markovflow.state_space_model.StateSpaceModel
      :property:

      Return the prior Gauss-Markov distribution.


   .. py:method:: loss() -> tensorflow.Tensor

      Return the loss, which is the negative ELBO.



.. py:class:: CVIGaussianProcess(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], kernel: markovflow.kernels.SDEKernel, likelihood: gpflow.likelihoods.Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None, learning_rate=0.1)

   Bases: :py:obj:`GaussianProcessWithSitesBase`

   Provides an alternative parameterization to a
   :class:`~markovflow.models.variational.VariationalGaussianProcess`.

   This class approximates the posterior of a model with a GP prior and a general likelihood
   using a Gaussian posterior parameterized with Gaussian sites.

   The following notation is used:

       * :math:`x` - the time points of the training data
       * :math:`y` - observations corresponding to time points :math:`x`
       * :math:`s(.)` - the latent state of the Markov chain
       * :math:`f(.)` - the noise free predictions of the model
       * :math:`p(y | f)` - the likelihood
       * :math:`t(f)` - a site (indices will refer to the associated data point)
       * :math:`p(.)` the prior distribution
       * :math:`q(.)` the variational distribution

   We use the state space formulation of Markovian Gaussian Processes that specifies:

   * The conditional density of neighbouring latent states :math:`p(sâ‚–â‚Šâ‚| sâ‚–)`
   * How to read out the latent process from these states :math:`fâ‚– = H sâ‚–`

   The likelihood links data to the latent process and :math:`p(yâ‚– | fâ‚–)`.
   We would like to approximate the posterior over the latent state space model of this model.

   To approximate the posterior, we maximise the evidence lower bound (ELBO) :math:`â„’` with
   respect to the parameters of the variational distribution, since:

   .. math:: log p(y) = â„’(q) + KL[q(s) â€– p(s | y)]

   ...where:

   .. math:: â„’(q) = âˆ« log(p(s, y) / q(s)) q(s) ds

   We parameterize the variational posterior through sites :math:`tâ‚–(fâ‚–)`:

   .. math:: q(s) = p(s) âˆâ‚– tâ‚–(fâ‚–)

   ...where :math:`tâ‚–(fâ‚–)` are univariate Gaussian sites parameterized in the natural form:

   .. math:: t(f) = exp(ğœ½áµ€Ï†(f) - A(ğœ½))

   ...and where :math:`ğœ½=[Î¸â‚,Î¸â‚‚]` and :math:`ğ›—(f)=[f,fÂ²]`.

   Here, :math:`ğ›—(f)` are the sufficient statistics and :math:`ğœ½` are the natural parameters.
   Note that the subscript :math:`k` has been omitted for simplicity.

   The natural gradient update of the sites can be shown to be the gradient of the
   variational expectations:

   .. math:: ğ  = âˆ‡[ğ°][âˆ« log(p(y=Y|f)) q(f) df]

   ...with respect to the expectation parameters:

   .. math:: ğ° = E[ğ›—(f)] = [Î¼, ÏƒÂ² + Î¼Â²]

   That is, :math:`ğœ½ â† Ïğœ½ + (1-Ï)ğ `, where :math:`Ï` is the learning rate.

   The key reference is::

     @inproceedings{khan2017conjugate,
       title={Conjugate-Computation Variational Inference: Converting Variational Inference
              in Non-Conjugate Models to Inferences in Conjugate Models},
       author={Khan, Mohammad and Lin, Wu},
       booktitle={Artificial Intelligence and Statistics},
       pages={878--887},
       year={2017}
     }

   :param input_data: A tuple containing the observed data:

       * Time points of observations with shape ``batch_shape + [num_data]``
       * Observations with shape ``batch_shape + [num_data, observation_dim]``

   :param kernel: A kernel that defines a prior over functions.
   :param likelihood: A likelihood with shape ``batch_shape + [num_inducing]``.
   :param mean_function: The mean function for the GP. Defaults to no mean function.
   :param learning_rate: The learning rate of the algorithm.

   .. py:method:: local_objective(Fmu: tensorflow.Tensor, Fvar: tensorflow.Tensor, Y: tensorflow.Tensor) -> tensorflow.Tensor

      Calculate local loss in CVI.

      :param Fmu: Means with shape ``[..., latent_dim]``.
      :param Fvar: Variances with shape ``[..., latent_dim]``.
      :param Y: Observations with shape ``[..., observation_dim]``.
      :return: A local objective with shape ``[...]``.


   .. py:method:: local_objective_and_gradients(Fmu: tensorflow.Tensor, Fvar: tensorflow.Tensor) -> tensorflow.Tensor

      Return the local objective and its gradients with regard to the expectation parameters.

      :param Fmu: Means :math:`Î¼` with shape ``[..., latent_dim]``.
      :param Fvar: Variances :math:`ÏƒÂ²` with shape ``[..., latent_dim]``.
      :return: A local objective and gradient with regard to :math:`[Î¼, ÏƒÂ² + Î¼Â²]`.


   .. py:method:: update_sites() -> None

      Perform one joint update of the Gaussian sites. That is:

      .. math:: ğœ½ â† Ïğœ½ + (1-Ï)ğ 


   .. py:method:: elbo() -> tensorflow.Tensor

      Calculate the evidence lower bound (ELBO) :math:`log p(y)`.

      This is done by computing the marginal of the model in which the likelihood terms were
      replaced by the Gaussian sites.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: classic_elbo() -> tensorflow.Tensor

      Compute the ELBO the classic way. That is:

      .. math:: â„’(q) = Î£áµ¢ âˆ« log(p(yáµ¢ | f)) q(f) df - KL[q(f) â€– p(f)]

      .. note:: This is mostly for testing purposes and should not be used for optimization.

      :return: A scalar tensor representing the ELBO.


   .. py:method:: predict_log_density(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], full_output_cov: bool = False) -> tensorflow.Tensor

      Compute the log density of the data at the new data points.
      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:
          a tensor of inputs with shape ``batch_shape + [num_data]``,
          a tensor of observations with shape ``batch_shape + [num_data, observation_dim]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).



.. py:function:: back_project_nats(nat1, nat2, C)

   Transforms the natural parameters ğœ½f of a Gaussian with sufficient statistics ğ›—(f)=[f,fÂ²]
   into equivalent rank one natural parameters ğœ½g of a (thus degenerate) Gaussian
   with sufficient statistics ğ›—(g)=[g,ggáµ€] where f = Cg.

   In practice  [Î¸gâ‚, Î¸gâ‚‚] = [Î¸fâ‚ C,Î¸fâ‚‚ Cáµ€C]

   :param nat1: natural parameters with size (num_time_points, 1)
   :param nat2: natural parameters with size (num_time_points, 1)
   :param C: projection with size (num_time_points, 1, project_dim)
   :return: natural parameters with size
           (num_time_points, project_dim)
           (num_time_points, project_dim, project_dim)



.. py:function:: gradient_transformation_mean_var_to_expectation(inputs: Tuple[tensorflow.Tensor, tensorflow.Tensor], grads: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

   Transform gradients.

   This is from :math:`ğ ` of a function with regard to :math:`[Î¼, ÏƒÂ²]` into its gradients
   with regard to :math:`[Î¼, ÏƒÂ² + Î¼Â²]`.

   :param inputs: Means and variances :math:`[Î¼, ÏƒÂ²]`.
   :param grads: Gradients :math:`ğ `.



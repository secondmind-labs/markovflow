:py:mod:`markovflow.models.models`
==================================

.. py:module:: markovflow.models.models

.. autoapi-nested-parse::

   Module containing base classes for models.

   .. note:: Markovflow models are intended to work with eager mode in TensorFlow. Therefore
      models (and their collaborating objects) should typically avoid performing any
      computation in their `__init__` methods. Because models and other objects are typically
      initialised outside of an optimisation loop, performing computation in the constructor
      means that this computation is performed 'too early', and optimisation is not possible.



Module Contents
---------------

.. py:class:: MarkovFlowModel(name=None)

   Bases: :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   Abstract class representing Markovflow models that depend on input data.

   All Markovflow models are :class:`TensorFlow Modules <tf.Module>`, so it is possible to obtain
   trainable variables via the :attr:`trainable_variables` attribute. You can combine this with
   the :meth:`loss` method to train the model. For example::

       optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
       for i in range(iterations):
           model.optimization_step(optimizer)

   Call the :meth:`predict_f` method to predict marginal function values at future time points.
   For example::

       mean, variance = model.predict_f(validation_data_tensor)

   .. note:: Markovflow models that extend this class must implement the :meth:`loss`
      method and :attr:`posterior` attribute.

   .. py:method:: loss() -> tensorflow.Tensor
      :abstractmethod:

      Obtain the loss, which you can use to train the model.
      It should always return a scalar.

      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: posterior() -> markovflow.posterior.PosteriorProcess
      :property:

      Return a posterior process from the model, which can be used for inference.

      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: predict_state(new_time_points: tensorflow.Tensor) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict state at `new_time_points`. Note these time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points,]``.
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, state_dim]``
          ``batch_shape + [num_new_time_points, state_dim, state_dim]``.


   .. py:method:: predict_f(new_time_points: tensorflow.Tensor, full_output_cov: bool = False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict marginal function values at `new_time_points`. Note these
      time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, output_dim]`` and either
          ``batch_shape + [num_new_time_points, output_dim, output_dim]`` or
          ``batch_shape + [num_new_time_points, output_dim]``.



.. py:class:: MarkovFlowSparseModel(name=None)

   Bases: :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   Abstract class representing Markovflow models that do not need to store the training
   data (:math:`X, Y`) in the model to approximate the
   posterior predictions :math:`p(f*|X, Y, x*)`.

   This currently applies only to sparse variational models.

   The `optimization_step` method should typically be used to train the model. For example::

       input_data = (tf.constant(time_points), tf.constant(observations))
       optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)
       for i in range(iterations):
           model.optimization_step(input_data, optimizer)

   Call the :meth:`predict_f` method to predict marginal function values at future time points.
   For example::

       mean, variance = model.predict_f(validation_data_tensor)

   .. note:: Markovflow models that extend this class must implement the :meth:`loss`
      method and :attr:`posterior` attribute.

   .. py:method:: loss(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> tensorflow.Tensor
      :abstractmethod:

      Obtain the loss, which can be used to train the model.

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: posterior() -> markovflow.posterior.PosteriorProcess
      :property:

      Obtain a posterior process from the model, which can be used for inference.

      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: predict_state(new_time_points: tensorflow.Tensor) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict state at `new_time_points`. Note these time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points,]``.
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, state_dim]``
          ``batch_shape + [num_new_time_points, state_dim, state_dim]``.


   .. py:method:: predict_f(new_time_points: tensorflow.Tensor, full_output_cov: bool = False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict marginal function values at `new_time_points`. Note these
      time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`FalseF`).
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, output_dim]`` and either
          ``batch_shape + [num_new_time_points, output_dim, output_dim]`` or
          ``batch_shape + [num_new_time_points, output_dim]``.


   .. py:method:: predict_log_density(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], full_output_cov: bool = False) -> tensorflow.Tensor

      Compute the log density of the data. That is:

      .. math:: log ∫ p(yᵢ=Yᵢ|Fᵢ)q(Fᵢ) dFᵢ

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).
      :return: Predicted log density at input time points, with shape
          ``batch_shape + [num_data]``.




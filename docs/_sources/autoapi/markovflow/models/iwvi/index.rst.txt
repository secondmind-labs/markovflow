:py:mod:`markovflow.models.iwvi`
================================

.. py:module:: markovflow.models.iwvi

.. autoapi-nested-parse::

   Module containing a model for importance-weighted variational inference.



Module Contents
---------------

.. py:class:: ImportanceWeightedVI(kernel: markovflow.kernels.SDEKernel, inducing_points: tensorflow.Tensor, likelihood: gpflow.likelihoods.Likelihood, num_importance_samples: int, initial_distribution: Optional[markovflow.state_space_model.StateSpaceModel] = None, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`markovflow.models.sparse_variational.SparseVariationalGaussianProcess`

   Performs importance-weighted variational inference (IWVI).

   The `key reference <https://papers.nips.cc/paper/7699-importance-weighting-and-
   variational-inference.pdf>`_ is::

       @inproceedings{domke2018importance,
         title={Importance weighting and variational inference},
         author={Domke, Justin and Sheldon, Daniel R},
         booktitle={Advances in neural information processing systems},
         pages={4470--4479},
         year={2018}
       }

   The idea is based on the observation that an estimator of the evidence lower bound (ELBO)
   can be obtained from an importance weight :math:`w`:

   .. math:: Lâ‚ = log w(xâ‚),    xâ‚ ~ q(x)

   ...where :math:`x` is the latent variable of the model (a GP, or set of GPs in our case)
   and the function :math:`w` is:

   .. math:: w(x) = p(y | x) p(x) / q(x)

   It follows that:

   .. math:: ELBO = ð”¼â‚“â‚[ Lâ‚ ]

   ...and:

   .. math:: log p(y) = log ð”¼â‚“â‚[ w(xâ‚) ]

   It turns out that there are a series of lower bounds given by taking multiple importance
   samples:

   .. math:: Lâ‚™ = log (1/n) Î£áµ¢â¿ w(xáµ¢),     xáµ¢ ~ q(x)

   And we have the relation:

   .. math:: log p(y) >= ð”¼[Lâ‚™] >= ð”¼[Lâ‚™â‚‹â‚] >= ... >= ð”¼[Lâ‚] = ELBO

   This means that we can improve tightness of the ELBO to the log marginal likelihood by
   increasing :math:`n`, which we refer to in this class as `num_importance_samples`.
   The trade-offs are:

       * The objective function is now always stochastic, even for cases where the ELBO
         of the parent class is non-stochastic
       * We have to do more computations (evaluate the weights :math:`n` times)

   :param kernel: A kernel that defines a prior over functions.
   :param inducing_points: The points in time on which inference should be performed,
       with shape ``batch_shape + [num_inducing]``.
   :param likelihood: A likelihood.
   :param num_importance_samples: The number of samples for the importance-weighted estimator.
   :param initial_distribution: An initial configuration for the variational distribution,
       with shape ``batch_shape + [num_inducing]``.
   :param mean_function: The mean function for the GP. Defaults to no mean function.

   .. py:method:: elbo(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> tensorflow.Tensor

      Compute the importance-weighted ELBO using K samples. The procedure is::

          for k=1...K:
              uâ‚– ~ q(u)
              sâ‚– ~ p(s | u)
              wâ‚– = p(y | sâ‚–)p(uâ‚–) / q(uâ‚–)

          ELBO = log (1/K) Î£â‚–wâ‚–

      Everything is computed in log-space for stability. Note that gradients
      of this ELBO may have high variance with regard to the variational parameters;
      see the DREGS gradient estimator method.

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :return: A scalar tensor.


   .. py:method:: dregs_objective(input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor]) -> tensorflow.Tensor

      Compute a scalar tensor that, when differentiated using `tf.gradients`,
      produces the DREGS variance controlled gradient.

      See `"Doubly Reparameterized Gradient Estimators For Monte Carlo Objectives"
      <https://openreview.net/pdf?id=HkG3e205K7>`_ for a derivation.

      We recommend using these gradients for training variational
      parameters and gradients of the importance-weighted ELBO for training hyperparameters.

      :param input_data: A tuple of time points and observations containing the data at which
          to calculate the loss for training the model:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :return: A scalar tensor.




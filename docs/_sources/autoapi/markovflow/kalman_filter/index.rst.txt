:py:mod:`markovflow.kalman_filter`
==================================

.. py:module:: markovflow.kalman_filter

.. autoapi-nested-parse::

   Module containing a Kalman filter.



Module Contents
---------------

.. py:class:: BaseKalmanFilter(state_space_model: markovflow.state_space_model.StateSpaceModel, emission_model: markovflow.emission_model.EmissionModel)

   Bases: :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   Performs a Kalman filter on a :class:`~markovflow.state_space_model.StateSpaceModel` and
   :class:`~markovflow.emission_model.EmissionModel`, with given observations.

   The key reference is::

       @inproceedings{grigorievskiy2017parallelizable,
           title={Parallelizable sparse inverse formulation Gaussian processes (SpInGP)},
           author={Grigorievskiy, Alexander and Lawrence, Neil and S{\"a}rkk{\"a}, Simo},
           booktitle={Int'l Workshop on Machine Learning for Signal Processing (MLSP)},
           pages={1--6},
           year={2017},
           organization={IEEE}
       }

   The following notation from the above paper is used:

       * :math:`G = I_N âŠ— H`, where :math:`âŠ—` is the Kronecker product
       * :math:`R` is the observation covariance
       * :math:`Î£ = I_N âŠ— R`
       * :math:`Kâ»Â¹ = Aâ»áµ€Qâ»Â¹Aâ»Â¹` is the precision, where :math:`Aâ»áµ€ =  [Aáµ€]â»Â¹ = [Aâ»Â¹]áµ€`
       * :math:`L` is the Cholesky of :math:`Kâ»Â¹ + Gáµ€Î£â»Â¹G`. That is, :math:`LLáµ€ = Kâ»Â¹ + Gáµ€Î£â»Â¹G`
       * :math:`y` is the observation matrix

   :param state_space_model: Parametrises the latent chain.
   :param emission_model: Maps the latent chain to the observations.

   .. py:method:: _r_inv()
      :property:

      Precision of observation model 


   .. py:method:: observations()
      :property:

      Observation vector 


   .. py:method:: _k_inv_prior()
      :property:

      Prior precision 


   .. py:method:: _k_inv_post()
      :property:

      Posterior precision 


   .. py:method:: _log_det_observation_precision()
      :property:

      Sum of log determinant of the precisions of the observation model 


   .. py:method:: posterior_state_space_model() -> markovflow.state_space_model.StateSpaceModel

      Return the posterior as a state space model.

      The marginal means and covariances are given by:

      .. math::
          &Î¼(Î§) = (Kâ»Â¹ + Gáµ€Î£â»Â¹G)â»Â¹[Gáµ€Î£â»Â¹y + Kâ»Â¹Î¼]\\
          &P(X) = Kâ»Â¹ + Gáµ€Î£â»Â¹G

      ...where :math:`Î¼` is a block vector of the marginal means.

      We can derive the state transitions :math:`aâ‚–` and process noise covariances :math:`qâ‚–`
      from the block tridiagonal matrix (see
      :meth:`~markovflow.block_tri_diag.SymmetricBlockTriDiagonal.upper_diagonal_lower`).
      Lower case is used to attempt to distinguish the posterior and prior parameters.

      We then need to calculate :math:`Î¼â‚€` and :math:`bâ‚–` (this is what most of the code in
      this function does). This can be calculated from:

      .. math:: Kâ»Â¹â‚šâ‚’â‚›â‚œÎ¼â‚šâ‚’â‚›â‚œ = Gáµ€Î£â»Â¹y + Kâ»Â¹â‚šáµ£áµ¢â‚’áµ£Î¼â‚šáµ£áµ¢â‚’áµ£

      Firstly, we use that for any :class:`~markovflow.state_space_model.StateSpaceModel`:

      .. math:: Kâ»Â¹Î¼ = Aâ»áµ€ Qâ»Â¹ m

      ...where :math:`m = [Î¼â‚€, bâ‚,... bâ‚™]` and::

          Aâ»Â¹ =  [ I             ]      Qâ»Â¹ =  [ Pâ‚€â»Â¹          ]
                 [-Aâ‚, I         ]            [    Qâ‚â»Â¹       ]
                 [    -Aâ‚‚, I     ]            [       á¨ž      ]
                 [         á¨ž  á¨ž  ]            [         á¨ž    ]
                 [         -Aâ‚™, I]            [           Qâ‚™â»Â¹]

      So:

      .. math:: mâ‚šâ‚’â‚›â‚œ = Qâ‚šâ‚’â‚›â‚œ Aâ‚šâ‚’â‚›â‚œáµ€ [Gáµ€Î£â»Â¹y + Kâ‚šáµ£áµ¢â‚’áµ£â»Â¹mâ‚šáµ£áµ¢â‚’áµ£]

      :return: The posterior as a state space model.


   .. py:method:: log_likelihood() -> tensorflow.Tensor

      Construct a TensorTlow function to compute the likelihood.

      We set :math:`y = obs - HÎ¼` (where :math:`Î¼` is the vector of marginal state means):

      .. math::
          log p(obs|params) = &- á´ºâ„â‚‚log(2Ï€) - Â½(log |Kâ»Â¹ + Gáµ€Î£â»Â¹G| - log |Kâ»Â¹| - log |Î£â»Â¹|)\\
                              &- Â½ yáµ€(Î£â»Â¹ - Î£â»Â¹G(Kâ»Â¹ + Gáµ€Î£â»Â¹G)â»Â¹Gáµ€Î£â»Â¹)y

      ...where :math:`N` is the dimensionality of the precision object, that is
      ``state_dim * (num_transitions + 1)``.

      We break up the log likelihood as: cst + term1 + term2 + term3. That is, as:

          * cst: :math:`- á´ºâ„â‚‚log(2Ï€)`
          * term 1: :math:`- Â½ yáµ€Î£â»Â¹y`
          * term 2:

            .. math::
               Â½ yáµ€Î£â»Â¹G(Kâ»Â¹ + Gáµ€Î£â»Â¹G)â»Â¹Gáµ€Î£â»Â¹)y = Â½ yáµ€Î£â»Â¹G(LLáµ€)â»Â¹Gáµ€Î£â»Â¹)y = Â½|Lâ»Â¹(Gáµ€Î£â»Â¹)y|Â²

          * term 3:

            .. math::
               - Â½(log |Kâ»Â¹ + Gáµ€Î£â»Â¹G| - log |Kâ»Â¹| - log |Î£â»Â¹|) = Â½log |Kâ»Â¹| - log |L| + Â½log |Î£â»Â¹|

      Note that there are a couple of mistakes in the SpinGP paper for this formula (18):

          * They have :math:`- Â½(... + log |Î£â»Â¹|)`. It should be :math:`- Â½(... - log |Î£â»Â¹|)`
          * They have :math:`- Â½ yáµ€(... Î£â»Â¹G(Kâ»Â¹ + Gáµ€Î£â»Â¹G)â»Â¹)y`. It should
            be :math:`- Â½ yáµ€(... Î£â»Â¹G(Kâ»Â¹ + Gáµ€Î£â»Â¹G)â»Â¹Gáµ€Î£â»Â¹)y`

      :return: The likelihood as a scalar tensor (we sum over the `batch_shape`).


   .. py:method:: _back_project_y_to_state(observations: tensorflow.Tensor) -> tensorflow.Tensor

      Back project from the observation space to the state_space, i.e. calculate (Gáµ€Î£â»Â¹)y.

      :param observations: a tensor y of shape
                  batch_shape + [num_data, output_dim]
      :return: a tensor (Gáµ€Î£â»Â¹)y of shape
                  batch_shape + [num_data, state_dim]



.. py:class:: KalmanFilter(state_space_model: markovflow.state_space_model.StateSpaceModel, emission_model: markovflow.emission_model.EmissionModel, observations: tensorflow.Tensor, chol_obs_covariance: gpflow.base.TensorType)

   Bases: :py:obj:`BaseKalmanFilter`

   Performs a Kalman filter on a :class:`~markovflow.state_space_model.StateSpaceModel` and
   :class:`~markovflow.emission_model.EmissionModel`, with given observations.

   The key reference is::

       @inproceedings{grigorievskiy2017parallelizable,
           title={Parallelizable sparse inverse formulation Gaussian processes (SpInGP)},
           author={Grigorievskiy, Alexander and Lawrence, Neil and S{\"a}rkk{\"a}, Simo},
           booktitle={Int'l Workshop on Machine Learning for Signal Processing (MLSP)},
           pages={1--6},
           year={2017},
           organization={IEEE}
       }

   The following notation from the above paper is used:

       * :math:`G = I_N âŠ— H`, where :math:`âŠ—` is the Kronecker product
       * :math:`R` is the observation covariance
       * :math:`Î£ = I_N âŠ— R`
       * :math:`Kâ»Â¹ = Aâ»áµ€Qâ»Â¹Aâ»Â¹` is the precision, where :math:`Aâ»áµ€ =  [Aáµ€]â»Â¹ = [Aâ»Â¹]áµ€`
       * :math:`L` is the Cholesky of :math:`Kâ»Â¹ + Gáµ€Î£â»Â¹G`. That is, :math:`LLáµ€ = Kâ»Â¹ + Gáµ€Î£â»Â¹G`
       * :math:`y` is the observation matrix

   :param state_space_model: Parametrises the latent chain.
   :param emission_model: Maps the latent chain to the observations.
   :param observations: Data with shape ``[num_transitions + 1, output_dim]``.
   :param chol_obs_covariance: A :data:`~markovflow.base.TensorType` with shape
       ``[output_dim, output_dim]`` for the Cholesky factor of the covariance to be
       applied to :math:`f` from `emission_model`.

   .. py:method:: _r_inv()
      :property:

      Precision of the observation model 


   .. py:method:: observations()
      :property:

      Observation vector 



.. py:class:: GaussianSites(name=None)

   Bases: :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   This class is a wrapper around the parameters specifying multiple independent
   Gaussian distributions.

   .. py:method:: means()
      :property:

      Return the means of the Gaussians.


   .. py:method:: precisions()
      :property:

      Return the precisions of the Gaussians.


   .. py:method:: log_det_precisions()
      :property:

      Return the sum of the log determinant of the observation precisions.



.. py:class:: UnivariateGaussianSitesNat(nat1, nat2, log_norm=None)

   Bases: :py:obj:`GaussianSites`

   This class is a wrapper around parameters of univariate Gaussian distributions
   in the natural form. That is:

   .. math:: p(f) = exp(ðž°áµ€Ï†(f) - A(ðž°))

   ...where :math:`ðž°=[Î·â‚,Î·â‚‚]` and :math:`ð›—(f)=[f,fÂ²]`.

   The mean :math:`Î¼` and variance :math:`ÏƒÂ²` parameterization is such that:

   .. math:: Î¼ = -Â½Î·â‚/Î·â‚‚, ÏƒÂ²=-Â½Î·â‚‚â»Â¹

   :param nat1: first natural parameter [N, D]
   :param nat2: second natural parameter [N, D, D]
   :param log_norm: normalizer parameter [N, D]

   .. py:method:: means()
      :property:

      Return the means of the Gaussians.


   .. py:method:: precisions()
      :property:

      Return the precisions of the Gaussians.


   .. py:method:: log_det_precisions()
      :property:

      Return the sum of the log determinant of the observation precisions. 



.. py:class:: KalmanFilterWithSites(state_space_model: markovflow.state_space_model.StateSpaceModel, emission_model: markovflow.emission_model.EmissionModel, sites: GaussianSites)

   Bases: :py:obj:`BaseKalmanFilter`

   Performs a Kalman filter on a :class:`~markovflow.state_space_model.StateSpaceModel` and
   :class:`~markovflow.emission_model.EmissionModel`, with Gaussian sites,
   that is time dependent Gaussian Likelihood terms.

   The key reference is::

       @inproceedings{grigorievskiy2017parallelizable,
           title={Parallelizable sparse inverse formulation Gaussian processes (SpInGP)},
           author={Grigorievskiy, Alexander and Lawrence, Neil and S{\"a}rkk{\"a}, Simo},
           booktitle={Int'l Workshop on Machine Learning for Signal Processing (MLSP)},
           pages={1--6},
           year={2017},
           organization={IEEE}
       }

   The following notation from the above paper is used:

       * :math:`G = I_N âŠ— H`, where :math:`âŠ—` is the Kronecker product
       * :math:`R = [Râ‚, Râ‚‚, ... Râ‚™]` is the observation covariance
       * :math:`Î£ = blockdiag[R]`
       * :math:`Kâ»Â¹ = Aâ»áµ€Qâ»Â¹Aâ»Â¹` is the precision, where :math:`Aâ»áµ€ =  [Aáµ€]â»Â¹ = [Aâ»Â¹]áµ€`
       * :math:`L` is the Cholesky of :math:`Kâ»Â¹ + Gáµ€Î£â»Â¹G`. That is, :math:`LLáµ€ = Kâ»Â¹ + Gáµ€Î£â»Â¹G`
       * :math:`y` is the observation matrix

   :param state_space_model: Parametrises the latent chain.
   :param emission_model: Maps the latent chain to the observations.
   :param sites: Gaussian sites parameterizing the Gaussian likelihoods.

   .. py:method:: _r_inv()
      :property:

      Precisions of the observation model 


   .. py:method:: _log_det_observation_precision()
      :property:

      Sum of log determinant of the precisions of the observation model 


   .. py:method:: observations()
      :property:

      Observation vector 



.. py:class:: KalmanFilterWithSparseSites(state_space_model: markovflow.state_space_model.StateSpaceModel, emission_model: markovflow.emission_model.EmissionModel, sites: GaussianSites, num_grid_points: int, observations_index: tensorflow.Tensor, observations: tensorflow.Tensor)

   Bases: :py:obj:`BaseKalmanFilter`

   Performs a Kalman filter on a :class:`~markovflow.state_space_model.StateSpaceModel`
   and :class:`~markovflow.emission_model.EmissionModel`, with Gaussian sites, over a time grid.

   :param state_space_model: Parameterises the latent chain.
   :param emission_model: Maps the latent chain to the observations.
   :param sites: Gaussian sites over the observations.
   :param num_grid_points: number of grid points.
   :param observations_index: Index of the observations in the time grid with shape (N,).
   :param observations: Sparse observations with shape [n_batch] + (N, output_dim).

   .. py:method:: _r_inv()
      :property:

      Precisions of the observation model over the time grid.


   .. py:method:: _drop_batch_shape(tensor: tensorflow.Tensor)

      Check the batch, if present, is equal to 1, and drop it.


   .. py:method:: _log_det_observation_precision()
      :property:

      Sum of log determinant of the precisions of the observation model. It only calculates for the data_sites as
      other sites precision is anyways zero.


   .. py:method:: observations()
      :property:

      Sparse observation vector 


   .. py:method:: _r_inv_data()
      :property:

      Precisions of the observation model for only the data sites.


   .. py:method:: sparse_to_dense(tensor: tensorflow.Tensor, output_shape: tensorflow.TensorShape) -> tensorflow.Tensor

      Convert a sparse tensor to a dense one on the basis of observations index, output tensor is of the output_shape.


   .. py:method:: dense_to_sparse(tensor: tensorflow.Tensor) -> tensorflow.Tensor

      Convert a dense tensor to a sparse one on the basis of observations index.


   .. py:method:: log_likelihood() -> tensorflow.Tensor

      Construct a TensorFlow function to compute the likelihood.

      For more mathematical details, look at the log_likelihood function of the parent class.
      The main difference from the parent class are that the vector of observations is now sparse.        

      :return: The likelihood as a scalar tensor (we sum over the `batch_shape`).




:py:mod:`markovflow.posterior`
==============================

.. py:module:: markovflow.posterior

.. autoapi-nested-parse::

   Module containing posterior processes for GP models.



Module Contents
---------------

.. py:class:: PosteriorProcess(name=None)

   Bases: :py:obj:`tensorflow.Module`, :py:obj:`abc.ABC`

   Abstract class for forming a posterior process.

   Posteriors that extend this class must implement the :meth:`sample_state_trajectories`,
   :meth:`sample_f`, :meth:`predict_state` and :meth:`predict_f` methods.

   .. py:method:: sample_state(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Optional[Tuple[tensorflow.Tensor, tensorflow.Tensor]] = None) -> tensorflow.Tensor

      Generate joint state samples at `new_time_points`.

      :param new_time_points: Time points to generate sample trajectories for, with shape
          ``batch_shape + [num_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape (or
          number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

          This is an optional argument only passed in for inference with an importance-weighted
          posterior.
      :return: A tensor containing:

          * Sampled trajectories at new points, with shape
            ``sample_shape + batch_shape + [num_time_points, state_dim]``
          * Sampled trajectories at conditioning points, with shape
            ``sample_shape + batch_shape + [num_conditioning_points, state_dim]``


   .. py:method:: sample_state_trajectories(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Optional[Tuple[tensorflow.Tensor, tensorflow.Tensor]] = None) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]
      :abstractmethod:

      Generate joint sampled state trajectories evaluated both at `new_time_points`
      and at some points that we condition on for obtaining the posterior.

      :param new_time_points: Time points to generate sample trajectories for, with shape
          ``batch_shape + [num_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape (or
          number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

          This is an optional argument only passed in for inference with an importance-weighted
          posterior.
      :return: A tensor containing:

          * Sampled trajectories at new points, with shape
            ``sample_shape + batch_shape + [num_time_points, state_dim]``
          * Sampled trajectories at conditioning points, with shape
            ``sample_shape + batch_shape + [num_conditioning_points, state_dim]``
      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: sample_f(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Optional[Tuple[tensorflow.Tensor, tensorflow.Tensor]] = None) -> tensorflow.Tensor
      :abstractmethod:

      Generate joint function evaluation samples (projected states) at `new_time_points`.

      :param new_time_points: Time points to generate sample trajectories for, with shape
          ``batch_shape + [num_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape (or
          number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

          This is an optional argument only passed in for inference with an importance-weighted
          posterior.
      :return: A tensor containing sampled trajectories, with shape
          ``sample_shape + batch_shape + [num_time_points, output_dim]``.
      :raises NotImplementedError: Must be implemented in derived classes.


   .. py:method:: predict_state(new_time_points: tensorflow.Tensor) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]
      :abstractmethod:

      Predict state at `new_time_points`. Note these time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points,]``.


   .. py:method:: predict_f(new_time_points: tensorflow.Tensor, full_output_cov: bool = False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]
      :abstractmethod:

      Predict marginal function values at `new_time_points`. Note these
      time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).



.. py:class:: ConditionalProcess(posterior_dist: markovflow.gauss_markov.GaussMarkovDistribution, kernel: markovflow.kernels.SDEKernel, conditioning_time_points: tensorflow.Tensor, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`PosteriorProcess`

   Represents a posterior process indexed on the real line.

   This means :math:`q(s(.))` is built by combining the marginals :math:`q(s(Z))` and the
   conditional process :math:`p(s(.)|s(Z))` into:

   .. math:: q(s(.)) = ∫p(s(.)|s(Z))q(s(Z)) ds(Z)

   The marginals at discrete time inputs are available in closed form
   (see the :meth:`predict_f` method).

   It also includes methods for sampling from the posterior process.

   :param posterior_dist: The posterior represented by a Gauss-Markov distribution used for
       inference. For variational models this is the model defined by the variational
       distribution.
   :param kernel: The kernel of the prior process.
   :param conditioning_time_points: The time points to condition on for inference, with shape
       ``batch_shape + [num_time_points]``.
   :param mean_function: The mean function of the process that is added to fs.

   .. py:method:: predict_state(new_time_points: tensorflow.Tensor) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict state at `new_time_points`. Note these time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points,]``.
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, state_dim]``
          ``batch_shape + [num_new_time_points, state_dim, state_dim]``.


   .. py:method:: predict_f(new_time_points: tensorflow.Tensor, full_output_cov: bool = False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict marginal function values at `new_time_points`. Note these
      time points should be sorted.

      .. note:: `new_time_points` that are far outside the `self.conditioning_time_points`
         specified when instantiating the class will revert to the prior.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, output_dim]`` and either
          ``batch_shape + [num_new_time_points, output_dim, output_dim]`` or
          ``batch_shape + [num_new_time_points, output_dim]``.


   .. py:method:: sample_state_trajectories(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Optional[Tuple[tensorflow.Tensor, tensorflow.Tensor]] = None) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Generate joint state samples at `new_time_points` and
      the `self.conditioning_time_points` specified when instantiating the class.

      See Appendix 2 of
      `"Doubly Sparse Variational Gaussian Processes" <https://arxiv.org/abs/2001.05363>`_
      for a derivation.

      The following notation is used:

          * :math:`t` - a vector of new time points
          * :math:`z` - a vector of the conditioning time points
          * :math:`sₚ/uₚ` - prior state sample at :math:`t/z`
          * :math:`sₒ/Uₒ` - posterior state sample at :math:`t/z`
          * :math:`p(.)` - the prior
          * :math:`q(.)` - the posterior

      Jointly sample from the prior at new and conditioning points:

      .. math:: [sₚ, uₚ] ~ p(s([t, z]))

      And sample from the posterior at the conditioning points:

      .. math:: uₒ ~ q(s(z))

      A sample from the posterior state is given by:

      .. math:: sₒ = sₚ - E[s(t) | s(z) = uₒ - uₚ]

      Noting :math:`z₋,z₊`, for each new point :math:`tₖ` the points in :math:`z` closest to
      :math:`tₖ` and :math:`vₖ = [s(z₋),s(z₊)]` are:

      .. math:: E[s(tₖ)|s(z)] = E[s(tₖ)|vₖ] = Pₖ vₖ

      That is, the conditional mean is local; it only depends on the nearing
      conditioning states.

      :param new_time_points: Time points to generate sample trajectories for, with shape
          ``batch_shape + [num_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape of sampled
          trajectories to draw. This can be either an integer or a tuple/list of integers.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

          Note this argument will be ignored if your posterior is an
          :class:`AnalyticPosteriorProcess`.
      :return: A tensor containing:

          * Sampled trajectories at new points, with shape
            ``sample_shape + batch_shape + [num_time_points, state_dim]``
          * Sampled trajectories at conditioning points, with shape
            ``sample_shape + batch_shape + [num_conditioning_points, state_dim]``


   .. py:method:: sample_f(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Optional[Tuple[tensorflow.Tensor, tensorflow.Tensor]] = None) -> tensorflow.Tensor

      Generate joint function evaluation samples (projected states) at `new_time_points`.

      :param new_time_points: Time points to generate sample trajectories for, with shape
          ``batch_shape + [num_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape
          (or number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

          Note this argument will be ignored if your posterior is an
          :class:`AnalyticPosteriorProcess`.
      :return: A tensor containing the sampled trajectories, with shape
          ``sample_shape + batch_shape + [num_time_points, output_dim]``.



.. py:class:: AnalyticPosteriorProcess(posterior_dist: markovflow.gauss_markov.GaussMarkovDistribution, kernel: markovflow.kernels.SDEKernel, conditioning_time_points: tensorflow.Tensor, likelihood: Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`ConditionalProcess`

   Represents the (approximate) posterior process of a GP model.

   It inherits the marginal prediction and sampling methods from the parent
   :class:`ConditionalProcess` class.

   It also includes a method to predict the observations (see :meth:`predict_y`).

   :param posterior_dist: The posterior represented by a Gauss-Markov distribution used
       for inference. For variational models this is the model defined by the variational
       distribution.
   :param kernel: The kernel of the prior process.
   :param conditioning_time_points: The time points to condition on for inference,
       with shape ``batch_shape + [num_time_points]``.
   :param likelihood: Likelihood defining how to project from f-space to an observation.
   :param mean_function: The mean function of the process that is added to fs.

   .. py:method:: predict_y(new_time_points: tensorflow.Tensor, full_output_cov=False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Predict observation marginals at `new_time_points`. Note these
      time points should be sorted.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).
      :return: Predicted mean and covariance for the new time points, with respective shapes
          ``batch_shape + [num_new_time_points, output_dim]`` and either
          ``batch_shape + [num_new_time_points, output_dim, output_dim]`` or
          ``batch_shape + [num_new_time_points, output_dim]``.



.. py:class:: ImportanceWeightedPosteriorProcess(num_importance_samples: int, proposal_dist: markovflow.gauss_markov.GaussMarkovDistribution, kernel: markovflow.kernels.SDEKernel, conditioning_time_points: tensorflow.Tensor, likelihood: Likelihood, mean_function: Optional[markovflow.mean_function.MeanFunction] = None)

   Bases: :py:obj:`PosteriorProcess`

   Represents the approximate posterior process of a GP model.

   The approximate posterior process is inferred via importance-weighted variational inference.

   :param num_importance_samples: The number of importance-weighted samples.
   :param proposal_dist: The proposal represented by a Gauss-Markov distribution,
       from which we draw samples. This is the model defined by the variational distribution.
   :param kernel: The kernel of the prior process.
   :param conditioning_time_points: Time points to condition on for inference, with shape
       ``batch_shape + [num_time_points]``.
   :param likelihood: Likelihood defining how to project from f-space to an observation.
   :param mean_function: The mean function of the process that is added to fs.

   .. py:method:: _log_qu_density(samples_u: tensorflow.Tensor, stop_gradient: bool = False)

      Log density of the posterior process evaluated at the conditioning points.

      :param samples_u: State samples at the conditioning time points, with shape
          ``sample_shape + [num_conditioning_points, state_dim]``.
      :param stop_gradient: Whether to stop the gradient flow through the samples. It is useful
          to do so when optimising the proposal distribution with control variates for reduced
          variance.
      :return: log q(u) [num_samples]


   .. py:method:: log_importance_weights(samples_s: tensorflow.Tensor, samples_u: tensorflow.Tensor, input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], stop_gradient: bool = False) -> tensorflow.Tensor

      Compute the log-importance weights for some state samples.

      The importance weights are given by:

      .. math:: w = p(Y | s) p(s, u) / q(s, u)

      Because it is assumed that :math:`q(s | u) = p(s | u)`, the weights reduce to:

      .. math:: w = p(Y | s) p(u) / q(u)

      We evaluate this ratio for some tensors of `samples_s` and `samples_u`, which
      are assumed to have been drawn from :math:`q(s, u)`. To do this, `samples_s` are
      projected to :math:`f` before being passed to the likelihood object.

      :param samples_s: A tensor of samples drawn from :math:`p(s|u)`, with shape
          ``sample_shape + batch_shape + [num_data, state_dim]``.
      :param samples_u: A tensor of samples drawn from :math:`q(u)`, with shape
          ``sample_shape + batch_shape + [num_inducing, state_dim]``.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :param stop_gradient: Whether to call stop gradient on :math:`q(u)`. This is
          useful for control variate schemes.
      :return log_weights: A tensor with shape ``[sample_shape]``.


   .. py:method:: _iwvi_samples_and_weights(new_time_points: tensorflow.Tensor, input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], sample_shape: markovflow.base.SampleShape) -> Tuple[tensorflow.Tensor, tensorflow.Tensor, tensorflow.Tensor]

      Sample from q(states) indexed by new_time_points and compute the log weights associated.

      :param new_time_points: ordered time input where to sample with shape
                      batch_shape + [num_new_time_points]
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``

      :param sample_shape: A :data:`~markovflow.base.SampleShape` that specifies how many
          samples to draw, with shape ``(..., num_importance_samples)``.
      :return: state samples from the posterior and the log-weights with shapes:
                      sample_shape + batch_shape + [num_new_time_points, state_dim]
                      sample_shape
                      sample_shape + batch_shape + [num_conditioning_points, state_dim]


   .. py:method:: sample_state_trajectories(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor] = None) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]

      Sample the importance-weighted posterior over states.

      :param new_time_points: Ordered time input from which to sample, with shape
          ``batch_shape + [num_new_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape
          (or number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :return: The ordered samples states, with shape
          ``sample_shape + batch_shape + [num_new_time_points, state_dim]``.


   .. py:method:: sample_f(new_time_points: tensorflow.Tensor, sample_shape: markovflow.base.SampleShape, *, input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor] = None) -> tensorflow.Tensor

      Sample the importance-weighted (IWVI) posterior over functions.

      Note that to compute the expected value of some function under the iwvi posterior,
      it is likely to be more efficient to use :meth:`expected_value`.

      :param new_time_points: Ordered time input from which to sample, with shape
          ``batch_shape + [num_new_time_points]``.
      :param sample_shape: A :data:`~markovflow.base.SampleShape` that is the shape
          (or number of) sampled trajectories to draw.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :return: The ordered samples on latent functions, with shape
          ``[num_samples] + batch_shape + [num_new_time_points, num_outputs]``.


   .. py:method:: expected_value(new_time_points: tensorflow.Tensor, input_data: Tuple[tensorflow.Tensor, tensorflow.Tensor], func=tf.identity) -> tensorflow.Tensor

      Compute the expected value of the function `func` acting on a random variable
      :math:`f`.

      :math:`f` is represented by a GP in this case, using importance sampling at the times
      given in `new_time_points`. That is:

      .. math:: ∫qₚ(f) func(f) df = Σₖ wₖ func(fₖ)

      ...where:

          * :math:`qₚ` is the importance-weighted approximate posterior distribution of :math:`f`
          * :math:`wₖ` are the importance weights

      For example, to compute the posterior mean we set `func = tf.identify`.

      :param new_time_points: Ordered time input from which to sample, with shape
          ``batch_shape + [num_new_time_points]``.
      :param input_data: A tuple of time points and observations containing the data:

          * A tensor of inputs with shape ``batch_shape + [num_data]``
          * A tensor of observations with shape ``batch_shape + [num_data, observation_dim]``
      :param func: The function to compute the expected value of. `func` should act
          on the last dimension of a tensor. That last dimension will have length as specified
          by the output_dim of the underlying emission model. The return shape of `func` need
          not be the same, but we expect all other dimensions to broadcast.
      :returns: A tensor with shape ``batch_shape + [num_new_time_points, output_dim]``.


   .. py:method:: predict_state(new_time_points: tensorflow.Tensor) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]
      :abstractmethod:

      Not applicable to :class:`ImportanceWeightedPosteriorProcess`.
      The marginal state predictions are not available in closed form.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points,]``.


   .. py:method:: predict_f(new_time_points: tensorflow.Tensor, full_output_cov: bool = False) -> Tuple[tensorflow.Tensor, tensorflow.Tensor]
      :abstractmethod:

      Not applicable to :class:`ImportanceWeightedPosteriorProcess`.
      The marginal function predictions are not available in closed form.

      :param new_time_points: Time points to generate observations for, with shape
          ``batch_shape + [num_new_time_points]``.
      :param full_output_cov: Either full output covariance (`True`) or marginal
          variances (`False`).



.. py:function:: _correct_mean_shape(mean: tensorflow.Tensor, kernel: markovflow.kernels.SDEKernel) -> tensorflow.Tensor

   Helper function that checks if the state space model is defined over a `StackKernel` so that it
   can bring the output of the mean function to the right shape.
   In any other case, the mean is returned unaltered.

   :param mean: the mean value that has been computed via a `MeanFunction`, with shape
                       batch_shape + [num_data, output_dim]
                    or batch_shape + [num_data, 1] in the case of a `StackKernel`,
                           where batch_shape[-1] = output_dim
   :param kernel: the corresponding kernel of the `GaussMarkovDistribution`

   :return: the mean value with the correct shape which is
           batch_shape + [num_data, output_dim] or
           batch_shape[:-1] + [num_data, output_dim] in the case of a `StackKernel` as the last
           dimension of the `batch_shape` is the output_dim.



{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "from gpflow.likelihoods import Gaussian\n",
    "\n",
    "from markovflow.models import SparseVariationalGaussianProcess\n",
    "from markovflow.kernels import Matern12, Matern32\n",
    "from markovflow.kernels.sde_kernel import IndependentMultiOutputStack\n",
    "from markovflow.ssm_natgrad import SSMNaturalGradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706d7d5",
   "metadata": {},
   "source": [
    "# Stacked kernels and multiple outputs\n",
    "This notebook is about _stacked kernels_, which is one way to get multiple outputs in MarkovFlow. \n",
    "\n",
    "Stacked kernels use a leading 'batch' dimension to compute multiple kernels together. Conceptually, if a kernel matrix is of dimensions `[N x N]`, then a stacked kernel produces an object of shape `[S x N x N]`. All of the markovflow computations will have this extra leading dimension. For example the state-transition matrices will be of shape `[S, T, D, D]`, where T is the number of time points and D is the state dimension. \n",
    "\n",
    "The _data_, however, are expected to be of shape `[N x S]`, so the `S` dimension should follow, not lead. This convention makes the stacked kernel compatible with likelihoods that can handle multiple outputs and processes. \n",
    "\n",
    "The advantage of this approach to multiple-outputs is that it is computationally efficient, because all computations can loop over this leading `S` dimension instead of augmenting the state dimension of the process. However, using a similar parameterization as an approximate posterior in an inference problem is a bit restrictive since it forces the posterior processes to be independent which may not always be an appropriate assumption.\n",
    " \n",
    "You may also be interesed in another style of multiple output kernels in MarkovFlow, where the state-dimensions of the processes are concatenated. In that case, the computational complexity grows cubically with the number of outputs, since the state dimension is growing. See the factor_analysis notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10f05a",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "We'll build a model with two outputs using a stacked kernel. We use the sparse GP object from markovflow to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "num_data = 300\n",
    "num_inducing = 50\n",
    "num_outputs = 2\n",
    "lengthscales = [0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94632ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a simple data set with correlated noise\n",
    "X = np.linspace(0, 1, num_data)\n",
    "X_tf = tf.broadcast_to(X, (num_outputs, num_data)) # duplicate\n",
    "F = np.hstack([np.sin(10 * X)[:, None], np.cos(15 * X)[:, None]])\n",
    "Sigma = np.array([[0.1, 0.08], [0.08, 0.1]])\n",
    "noise = np.random.multivariate_normal(np.zeros(2), Sigma, num_data)\n",
    "Y = F + noise\n",
    "data = (X_tf, tf.convert_to_tensor(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ab318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constuct a stacked kernel with two outputs\n",
    "k1 = Matern12(lengthscale=lengthscales[0], variance=1.)\n",
    "k2 = Matern32(lengthscale=lengthscales[1], variance=1.)\n",
    "kern = IndependentMultiOutputStack([k1, k2], jitter=1e-6)\n",
    "\n",
    "# construct a model\n",
    "lik = Gaussian(1.)\n",
    "Z = tf.broadcast_to(np.linspace(0, 1, num_inducing), (num_outputs, num_inducing))\n",
    "m = SparseVariationalGaussianProcess(kern, lik, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c356d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    # plot the data with predictions:\n",
    "    p = m.posterior\n",
    "    mu, var = p.predict_y(X_tf)\n",
    "    for i in [0, 1]:\n",
    "        plt.plot(X, Y[:, i], f'C{i}x', alpha=0.5)\n",
    "        plt.plot(X, mu[:, i], f'C{i}')\n",
    "        std = tf.math.sqrt(var[:, i])\n",
    "        plt.plot(X, mu[:, i] + 2 * std, f'C{i}--')\n",
    "        plt.plot(X, mu[:, i] - 2 * std, f'C{i}--')\n",
    "\n",
    "plot()\n",
    "_ = plt.title('The model fit before optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model):\n",
    "    # we'll use the natural gradient optimizer for the variational parameters and the Adam optimizer for hyper-parameters\n",
    "    opt_ng = SSMNaturalGradient(.5)\n",
    "    opt_adam = tf.optimizers.Adam(0.05)\n",
    "\n",
    "    @tf.function\n",
    "    def step():\n",
    "        opt_adam.minimize(lambda : -model.elbo(data), model._likelihood.trainable_variables + model._kernel.trainable_variables)\n",
    "        opt_ng.minimize(lambda : -model.elbo(data), ssm=m.dist_q)\n",
    "    \n",
    "    @tf.function\n",
    "    def elbo():\n",
    "        return m.elbo(data)\n",
    "    \n",
    "    max_iter = ci_niter(400)\n",
    "    for i in range(max_iter):\n",
    "        step()\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}, elbo:{elbo().numpy():.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()\n",
    "_ = plt.title('The model fit after optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813c165",
   "metadata": {},
   "source": [
    "## What about broadcasting?\n",
    "Since we're using a leading dimension in the stacked kernel, we might be worried about whether this impedes markovflow's ability to fit a model to multiple independent datasets. Fear not! extra leading dimensions are still handled (and looped over appropriately), and parameter sharing of the kernels (between datasets, not outputs) still happens smoothly. \n",
    "\n",
    "In this example, we'll fit a model with a heteroskedastic likelihood to multiple datasets simultaneously. The likelihood requires two GP outputs to model a *single* data column. One of the GPs models the mean of the data, and the other models the variance. We'll generate multiple datasets, construct the outline of a very simple likelihood and fit the whole shebang in a single markovflow model. Each dataset gets its own GPs, but the kernels paraemters are shared amongst datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e72000",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 300\n",
    "num_datasets = 2\n",
    "num_inducing = 30\n",
    "num_gp_outputs = 2\n",
    "num_data_outputs = 1\n",
    "lengthscales = [0.05, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307be08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets from sinusoidal means and time varying noise variances (exponentiated sinusoids)\n",
    "Xs, Ys = [], []\n",
    "for d in range(num_datasets):\n",
    "    X = np.linspace(0, 1, num_data)\n",
    "    amplitudes = np.random.rand(2) * np.array([1, 0.5]) + np.array([3, 2])\n",
    "    phases = np.random.randn(2) * 2 * np.pi\n",
    "    frequencies =  np.array([10, 2])\n",
    "    f1, f2 = [np.sin(2*np.pi * X * omega + phi) * a for omega, phi, a in zip(frequencies, phases, amplitudes)]\n",
    "    Y = f1 + np.random.randn(*f2.shape) * np.exp(0.5 * f2)\n",
    "    Ys.append(Y.reshape(num_data, num_data_outputs))\n",
    "    Xs.append(tf.broadcast_to(X, (num_gp_outputs, num_data)))\n",
    "\n",
    "Xs = tf.convert_to_tensor(Xs)  # [num_datasets, num_gps, num_data]\n",
    "Ys = tf.convert_to_tensor(Ys)  # [num_datasets, num_data, num_data_outputs]\n",
    "data = (Xs, Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from markovflow.likelihoods import Likelihood\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class HetGaussian(Likelihood):\n",
    "    def log_probability_density(self, f, y):\n",
    "        mu, logvar = f[..., 0], f[..., 1]\n",
    "        return tfp.distributions.Normal(loc=mu, scale=tf.exp(0.5 * logvar)).log_p(y[..., 0])\n",
    "    \n",
    "    def variational_expectations(self, f_means, f_covariances, observations):\n",
    "        f1, f2 = f_means[..., 0], f_means[..., 1]\n",
    "        variances = f_covariances # assume independent GPs\n",
    "        v1, v2 = variances[..., 0], variances[..., 1]\n",
    "        return -0.5 * (np.log(2*np.pi) + f2 + \n",
    "                       tf.exp(-f2 + 0.5 * v2) * (tf.square(f1 - observations[..., 0]) + v1))\n",
    "    def predict_density(self, f_means, f_covariances, observations):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict_mean_and_var(self, f_means, f_covariances):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0301ed6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# constuct a stacked kernel with two outputs\n",
    "k1 = Matern32(lengthscale=.05, variance=1.)\n",
    "k2 = Matern12(lengthscale=.5, variance=1.)\n",
    "kern = IndependentMultiOutputStack([k1, k2])\n",
    "\n",
    "# construct a model\n",
    "lik = HetGaussian()\n",
    "Z = tf.broadcast_to(np.linspace(0, 1, num_inducing), (num_datasets, num_gp_outputs, num_inducing))\n",
    "m = SparseVariationalGaussianProcess(kern, lik, Z)\n",
    "\n",
    "print(m.elbo(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow.optimizers import Scipy\n",
    "opt = Scipy()\n",
    "opt.minimize(lambda: -m.elbo(data), m.trainable_variables, options=dict(maxiter=ci_niter(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mus, _ = m.posterior.predict_f(Xs)\n",
    "\n",
    "f, axes = plt.subplots(num_datasets, 1, sharex=True, sharey=True, figsize=(8, 6))\n",
    "for i, (Y, ax, mu) in enumerate(zip(Ys, axes, mus)):\n",
    "    ax.plot(Y, 'C0.', alpha=0.3)\n",
    "    ax.plot(mu[:, 0], 'C1')\n",
    "    ax.plot(mu[:, 0] + 2 * tf.exp(0.5 * mu[:, 1]), 'C1--')\n",
    "    ax.plot(mu[:, 0] - 2 * tf.exp(0.5 * mu[:, 1]), 'C1--')\n",
    "    ax.set_title(f'dataset {i}')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

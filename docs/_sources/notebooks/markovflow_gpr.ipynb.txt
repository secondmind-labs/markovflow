{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d9d90f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Basic regression using the GPR model\n",
    "\n",
    "GMRF (Gaussian Markov Random Fields) correspond to Gaussian Process models that are parametrised by the inverse of the covariance: the precision.\n",
    "\n",
    "This notebook explains how to use Markovflow to build and optimise a GP regression model for a time series. **NOTE:** Markovflow does not require that the observations in a time series are regularly spaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpflow import default_float\n",
    "from gpflow.ci_utils import ci_niter\n",
    "\n",
    "from markovflow.kernels import Matern32\n",
    "from markovflow.models import GaussianProcessRegression\n",
    "\n",
    "FLOAT_TYPE = default_float()\n",
    "\n",
    "# Turn off warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "except AttributeError:\n",
    "    print('Magic function can only be used in IPython environment')\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef674da7",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 1: Generate training data\n",
    "Usually it is a good idea to normalise the data, because by default most kernels revert to a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc9c19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def create_observations(time_points: np.ndarray) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    A helper function to create training data.\n",
    "    :param time_points: Points in time.\n",
    "    :return: Tuple[x,y] Data that represents the observation shapes:\n",
    "        X = [num_points, 1],\n",
    "        Y = [num_points, state_dim , 1] where state_dim is currently 1\n",
    "    \"\"\"\n",
    "    observations = np.sin(12 * time_points[..., None])\n",
    "    observations += np.random.randn(len(time_points), 1) * 0.1\n",
    "    observations += 3\n",
    "    return time_points, observations\n",
    "\n",
    "# Generate some observations\n",
    "time_points, observations = create_observations(np.arange(5.0, 20.0))\n",
    "\n",
    "# Usually it is a good idea to normalise the data, because by default most kernels revert to a mean of zero\n",
    "norm_observations = (observations - np.mean(observations)) / np.std(observations)\n",
    "\n",
    "plt.plot(time_points.squeeze(), norm_observations.squeeze(), 'C0x')\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d766f74",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 2: Choose a kernel and create the model\n",
    "Markovflow provides several SDE (Stochastic Differential Equation) kernels. Your domain knowledge is encoded in your choice of kernel, or combination of kernels. For more information, see\n",
    "[Choosing and combining kernels](./choosing_and_combining_kernels.ipynb).\n",
    "\n",
    "In this example we use a Matern 1/2 kernel. However, given our knowledge of the data, a periodic kernel might be more appropriate. Why not try it?\n",
    "\n",
    "We also use an observation covariance of 0.001. The observation covariance is the amount of noise that we believe exists in our observations (the measurement noise).\n",
    "\n",
    "**NOTE:** In this example `observation_covariance` is not set as trainable.  This is the equivalent of specifying a specific value for the measurement noise. Try varying the magnitude of this parameter, or making it trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0319030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some noise to the observations\n",
    "observation_covariance = tf.constant([[0.0001]], dtype=FLOAT_TYPE)\n",
    "\n",
    "# Create a GPR model\n",
    "kernel = Matern32(lengthscale=8.0, variance=1.0)\n",
    "input_data = (tf.constant(time_points), tf.constant(norm_observations))\n",
    "gpr = GaussianProcessRegression(input_data=input_data, kernel=kernel,\n",
    "                                chol_obs_covariance=tf.linalg.cholesky(observation_covariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a56ab9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "We can calculate the marginal likelihood (that is, the probability of the observed data given the model) before we optimise the model.\n",
    "**NOTE:** We are using log likelihood, so a probability of 1.0 (the data definitely came from the model) is equal to a log likelihood of zero (with lower probabilities increasingly negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af902c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before optimisation, calculate the log likelihood of the observations given the current kernel parameters \n",
    "print(gpr.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ab137",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "After optimisation, the probability of the data given the model should have increased (that is, the log likelihood should have increased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a33122",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def opt_step():\n",
    "    opt.minimize(gpr.loss, gpr.trainable_variables)\n",
    "\n",
    "max_iter = ci_niter(4000)\n",
    "for _ in range(max_iter):\n",
    "    opt_step()\n",
    "\n",
    "print(gpr.log_likelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048971a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 3: Generate a mean for the training data\n",
    "We can use the model's posterior `predict_f` function to get the mean of the true function values (observations without noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c77147",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, _ = gpr.posterior.predict_f(gpr.time_points)\n",
    "# Plot the results\n",
    "plt.plot(time_points, norm_observations, 'C0x')\n",
    "plt.plot(time_points, mean, mew=2)\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87daec",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 4: Make a prediction for the future\n",
    "The GPR model's `posterior` supports interpolation and extrapolation of the underlying state-space model.\n",
    "\n",
    "For example, the `gpr.posterior.predict_f` function predicts means and covariances for the specified time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67be081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some time points in the future\n",
    "future_time_points = np.arange(time_points[-1] + 0.01, time_points[-1] + 10.0, 0.1)\n",
    "\n",
    "predicted_mean, predicted_cov = \\\n",
    "    gpr.posterior.predict_f(tf.constant(future_time_points, dtype=FLOAT_TYPE))\n",
    "predicted_mean, predicted_cov = predicted_mean.numpy(), predicted_cov.numpy()\n",
    "\n",
    "# Plot the means and covariances for these future time points\n",
    "plt.plot(time_points, mean, mew=2)\n",
    "plt.plot(future_time_points, predicted_mean, 'C0', lw=2)\n",
    "plt.fill_between(future_time_points,\n",
    "                 predicted_mean[:, 0] - 2 * np.sqrt(predicted_cov[:, 0]),\n",
    "                 predicted_mean[:, 0] + 2 * np.sqrt(predicted_cov[:, 0]),\n",
    "                 color='C0', alpha=0.2)\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c5a31",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "The `gpr.posterior.sample_f` function samples from the posterior probability distribution. Note the variance of the initial points of the generated sampled trajectories. This is a result of the observation covariance we specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gpr.posterior.sample_f(tf.constant(future_time_points, dtype=FLOAT_TYPE), 50)\n",
    "\n",
    "# Plot the same as previous\n",
    "plt.plot(time_points, mean, mew=2)\n",
    "plt.plot(future_time_points, predicted_mean, 'C0', lw=2)\n",
    "plt.fill_between(future_time_points,\n",
    "                 predicted_mean.squeeze() - 2 * np.sqrt(predicted_cov.squeeze()),\n",
    "                 predicted_mean.squeeze() + 2 * np.sqrt(predicted_cov.squeeze()),\n",
    "                 color='C0', alpha=0.2)\n",
    "# Add the samples\n",
    "plt.plot(future_time_points[..., None], np.swapaxes(samples, 0, 1).squeeze())\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd47d3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 5: Show a history of confidence levels\n",
    "The `gpr.posterior.predict_f` gets the posterior of the latent function at arbitrary time points.\n",
    "To demonstrate this, we can generate a set of time points that begin before the training data and\n",
    "extend them into the future. Note how the model is very certain about the fit in the region where there\n",
    "is data (the confidence intervals are small), whereas the uncertainty grows when we predict in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some other time points to evaluate\n",
    "intermediate_time_points = np.arange(0.0, time_points[-1] + 10.0, 0.1)\n",
    "predicted_mean, predicted_cov = \\\n",
    "    gpr.posterior.predict_f(tf.constant(intermediate_time_points, dtype=FLOAT_TYPE))\n",
    "predicted_mean, predicted_cov = predicted_mean.numpy(), predicted_cov.numpy()\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(intermediate_time_points, predicted_mean, 'C0', lw=2)\n",
    "plt.fill_between(intermediate_time_points,\n",
    "                 predicted_mean.squeeze() - 2 * np.sqrt(predicted_cov.squeeze()),\n",
    "                 predicted_mean.squeeze() + 2 * np.sqrt(predicted_cov.squeeze()),\n",
    "                 color='C0', alpha=0.2)\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0713467",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 6: Observe more data in the future\n",
    "When new data becomes available, we can see how the variance collapses (the confidence increases) at the new point (`t=23`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a275f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time, new_ob = create_observations(np.array([23.]))\n",
    "# Normalise the new point based on the original observation's mean and std\n",
    "new_ob = (new_ob - np.mean(observations)) / np.std(observations)\n",
    "time_points = np.concatenate([time_points, new_time], axis=0)\n",
    "new_observations = np.concatenate([norm_observations, new_ob], axis=0)\n",
    "\n",
    "gpr._time_points = tf.constant(time_points)\n",
    "gpr._observations = tf.constant(new_observations)\n",
    "\n",
    "predicted_mean, predicted_cov = \\\n",
    "    gpr.posterior.predict_f(tf.constant(intermediate_time_points, dtype=FLOAT_TYPE))\n",
    "# Plot the results\n",
    "plt.plot(intermediate_time_points, predicted_mean, 'C0', lw=2)\n",
    "plt.fill_between(intermediate_time_points,\n",
    "                 predicted_mean[:, 0] - 2 * np.sqrt(predicted_cov[:, 0]),\n",
    "                 predicted_mean[:, 0] + 2 * np.sqrt(predicted_cov[:, 0]),\n",
    "                 color='C0', alpha=0.2)\n",
    "plt.xlim((0., 30.))\n",
    "plt.ylim((-3., 3.))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\"",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
